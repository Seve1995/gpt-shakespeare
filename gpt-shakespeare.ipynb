{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This project follows this tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY, and has the goal to practice with the creation of a simple GPT model to generate Shakespeare's like output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the input**\n",
    "\n",
    "As an input, we will use a raw text containing Shakespeare's dialogues"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n",
      "100 1089k  100 1089k    0     0   178k      0  0:00:06  0:00:06 --:--:--  224k\n"
     ]
    }
   ],
   "source": [
    "# Start by downloading the file\n",
    "!curl -o input.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.671717Z",
     "start_time": "2023-09-04T15:25:08.523156300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read input to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.726570800Z",
     "start_time": "2023-09-04T15:25:14.672714800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print('Length of dataset in characters: ', len(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.738539100Z",
     "start_time": "2023-09-04T15:25:14.719590400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.748512700Z",
     "start_time": "2023-09-04T15:25:14.733552200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Create a set of all chars in the text, then create a sorted list out of it\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.818325600Z",
     "start_time": "2023-09-04T15:25:14.751504700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenize the characters**\n",
    "\n",
    "This means converting the raw text, as a string, to some sequence of integers, according to some vocabulary of possible elements."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping characters to their corresponding indices.\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Create a dictionary mapping indices to their corresponding characters.\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode a string into a list of integers using the character-to-index mapping.\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "\n",
    "# Decode a list of integers into a string using the index-to-character mapping.\n",
    "decode = lambda l: ''.join(itos[i] for i in l)\n",
    "\n",
    "print(encode(\"Hello world\"))\n",
    "print(decode(encode(\"Hello world\")))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:14.819323600Z",
     "start_time": "2023-09-04T15:25:14.779430500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:25.876937600Z",
     "start_time": "2023-09-04T15:25:14.795389100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Splitting the dataset into training and validation sets**\n",
    "\n",
    "An important step is to split the data into train and validation sets. We will use 90% of the data as training, and the rest as validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:25.919641600Z",
     "start_time": "2023-09-04T15:25:25.876937600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data loader**\n",
    "\n",
    "We'll not feed entire text into transformer all at once, it will be computationally very expensive.\n",
    "We only work with chunks of the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8 # Chunk size\n",
    "train_data[:block_size]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:25.950559300Z",
     "start_time": "2023-09-04T15:25:25.892713500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size] # Input to the transformers\n",
    "y = train_data[1:block_size+1] # Next block_size characters, so it's offset by 1 in comparison to x\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:25.951584100Z",
     "start_time": "2023-09-04T15:25:25.907673700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What we've just seen is useful for the transformer: we start the sampling generation with as little as one character of context, so that the transformer knows how to predict the next character with all the way up from just one character up to block size. After block size, we need to start truncating, because the transformer will never receive more than block_size inputs when it's predicting the next character."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Batch dimension**\n",
    "\n",
    "We'll feed the transformer with batches of multiple chunks of text, for efficiency reasons"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "-----\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337) # Set this manually to match the tutorial's samples\n",
    "batch_size = 4 # How many independent sequences will we process in parallel?\n",
    "block_size = 8 # What is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Generate batch_size numbers between 0 (inclusive) and (len(data)-block_size) (exclusive)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Stack the rows\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "for b in range(batch_size): # Batch dimension\n",
    "    for t in range(block_size): # Time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print (f\"When input is {context.tolist()} the target is: {target}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:26.025359500Z",
     "start_time": "2023-09-04T15:25:25.923631100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # Our input to the transformer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:26.025359500Z",
     "start_time": "2023-09-04T15:25:25.984469Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Bigram Language Model**\n",
    "\n",
    "We will start implementing a simple Neural Network, by using the Bigram Language Model.\n",
    "A Bigram Language Model is a type of statistical language model used in natural language processing and computational linguistics. The main idea behind a Bigram Language Model is to predict the likelihood of the next word in a sequence (i.e., the next token) given the previous word (the preceding token). It assumes that the probability of a word depends only on its immediate predecessor.\n",
    "Bigram models are straightforward and computationally less intensive compared to more complex language models like n-gram models or neural language models. However, they have limitations due to their local context dependency, which means they cannot capture long-range dependencies or semantics effectively.\n",
    "\n",
    "Every single integer in our input is going to refer to the embedding table and is going to have a row of the table corresponding to its index.\n",
    "E.g. integer '24' will be associated to row 24.\n",
    "Pytorch will arrange then this in a B x T x C (Batch x Time x Channels) tensor.\n",
    "Batch = 4\n",
    "Time = 8\n",
    "Channels = 65 (it's the vocab_size)\n",
    "This tensor will contain the logits (=scores) for the next character in the sequence\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) tensor, containing the logits (=scores) for the next character in the sequence\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Reshaping the logits because the cross_entropy function expect a (B,C,T) matrix\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # Stretch the tensor to be 2-dimensional\n",
    "            targets = targets.view(B*T) # Stretch the tensor to be 1-dimensional\n",
    "            loss = F.cross_entropy(logits, targets) # Negative log likelihood loss function. This measure the quality of the logits w.r.t. the targets\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # Focus only on the last time step (so we're ignoring all the previous token, but just considering the very last one)\n",
    "            logits = logits[:, -1, :] # Becomes (B,C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb) # Pass the inputs and the targets\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "idx = torch.zeros((1,1), dtype = torch.long) # This 1x1 tensor contains 0, that will be used to kick off the generation. Remember that in our vocabulary 0 is the ' ' character\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist())) # Ask 100 tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:26.115118100Z",
     "start_time": "2023-09-04T15:25:26.013418700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train the model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:26.115118100Z",
     "start_time": "2023-09-04T15:25:26.081209300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5589075088500977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # Update batch_size to 32 instead of 4\n",
    "for steps in range(10000):\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.082416700Z",
     "start_time": "2023-09-04T15:25:26.105144800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ong h hasbe pave pirance\n",
      "RDe hicomyonthar's\n",
      "PES:\n",
      "AKEd ith henourzincenonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KINld pe wither vouprroutherccnohathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so itJas\n",
      "Waketancotha:\n",
      "h hay.JUCLUKn prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZEESTEORDY:\n",
      "h l.\n",
      "KEONGBUCHandspo be y,-JZNEEYowddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghienHen yof GLANCHI me. strsithisgothers jveere!-e!\n",
      "QUCotouciullle's fldrwertho s?\n",
      "NDan'spererds cist ripl chyreer orlese;\n",
      "Yo jowof h hecere ek? wferommot mowo soaf you f;\n",
      "Ane his, t, f at. fal whetrimy bupof tor atha Bu!\n",
      "JOutho fplimimave.\n",
      "NEDUSt cir selle p wie wede\n",
      "Ro n apenor f'Y tover witys an sh d w t e w!\n",
      "CilttiretoaveE IINGAwe n ck. cung.\n",
      "ORDUSURes hacin benqurd bll, d a r w wistatsowor ath\n",
      "Fivet bloll ang aror;\n",
      "ARKIOULemee tsce larry t I Ane szF t\n",
      "LCay thit,\n",
      "n.\n",
      "Faure ds ppplirn!\n",
      "Whotou ow pyofalondrwist th;thomayo war gmenco, An he waro whiougou he s imaror\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist())) # Ask 1000 tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.243983800Z",
     "start_time": "2023-09-04T15:25:42.084411100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The mathematical trick in self-attention**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 8, 2])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,2 # Batch, Time, Channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.253957200Z",
     "start_time": "2023-09-04T15:25:42.238000100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to make sure that tokens only communicate with previous tokens, and not with the one that are in the future.\n",
    "E.g. token number 5 needs to communicate to token number 1,2,3,4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow = bag of words\n",
    "for b in range(B): # Iterate over all batches\n",
    "    for t in range(T): # Iterate over time\n",
    "        xprev = x[b,:t+1] # Previous tokens are up to, and including, the t-th token # It has shape (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0) # Average over the 0-th dimension, that is time. This will be a c-shaped dimensional vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.312800200Z",
     "start_time": "2023-09-04T15:25:42.254954800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1808, -0.0700],\n        [-0.3596, -0.9152],\n        [ 0.6258,  0.0255],\n        [ 0.9545,  0.0643],\n        [ 0.3612,  1.1679],\n        [-1.3499, -0.5102],\n        [ 0.2360, -0.2398],\n        [-0.9211,  1.5433]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.324767600Z",
     "start_time": "2023-09-04T15:25:42.268917600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1808, -0.0700],\n        [-0.0894, -0.4926],\n        [ 0.1490, -0.3199],\n        [ 0.3504, -0.2238],\n        [ 0.3525,  0.0545],\n        [ 0.0688, -0.0396],\n        [ 0.0927, -0.0682],\n        [-0.0341,  0.1332]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.324767600Z",
     "start_time": "2023-09-04T15:25:42.285872400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first element is equal between x[0] and xbow[0], because we are doing an average of a single element.\n",
    "But the 2nd element of xbow is an average between the 1st and the 2nd element of x[0].\n",
    "The last element will be the average of all the elements of x[0].\n",
    "\n",
    "This is inneficient, so we need a mathematical trick to use Matrix multiplication. Let's start with an example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # Normal matrix multiplication\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.331749400Z",
     "start_time": "2023-09-04T15:25:42.300832200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0.],\n        [1., 1., 0.],\n        [1., 1., 1.]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3)) # Trill = Triangular, it will return the lower triangolar portion of the original matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.405552200Z",
     "start_time": "2023-09-04T15:25:42.331749400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What if we apply that to the previous example?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # Normal matrix multiplication\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.405552200Z",
     "start_time": "2023-09-04T15:25:42.361670700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c contains, in the first row, exactly the first row of b.\n",
    "On the 2nd row, we will have (2+6) and (7+4).\n",
    "On the 3rd row, we will have (2+6+6) and (7+4+5).\n",
    "So we are just summing the element of b by incrementing its number one by one.\n",
    "We can use this approach to also calculate the average in an incremental fashion.\n",
    "We just need to normalize the rows of a in order to always have sum = 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # Normal matrix multiplication\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.407546800Z",
     "start_time": "2023-09-04T15:25:42.379621600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is exactly what we did before with the two for cycle, so we can use this approach to have a more efficient code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Version 2: using matrix multiply**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T)) # Wei = weights\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.417519500Z",
     "start_time": "2023-09-04T15:25:42.392586500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x # (T,T) @ (B,T,C) -> PyTorch will transform the first matrix in (B,T,T) to match the dimensions. So, for each batch element, there will be a (T,T) multiplying a (T,C), exactly as we had in the previous for cycle, because it will return a (B,T,C) matrix\n",
    "torch.allclose(xbow, xbow2) # checks whether all elements of two tensors, xbow and xbow2, are close within a certain tolerance. It returns a boolean value indicating whether the condition is true for all elements or not."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.514260900Z",
     "start_time": "2023-09-04T15:25:42.409541500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[ 0.1808, -0.0700],\n         [-0.0894, -0.4926],\n         [ 0.1490, -0.3199],\n         [ 0.3504, -0.2238],\n         [ 0.3525,  0.0545],\n         [ 0.0688, -0.0396],\n         [ 0.0927, -0.0682],\n         [-0.0341,  0.1332]]),\n tensor([[ 0.1808, -0.0700],\n         [-0.0894, -0.4926],\n         [ 0.1490, -0.3199],\n         [ 0.3504, -0.2238],\n         [ 0.3525,  0.0545],\n         [ 0.0688, -0.0396],\n         [ 0.0927, -0.0682],\n         [-0.0341,  0.1332]]))"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0], xbow2[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.515258400Z",
     "start_time": "2023-09-04T15:25:42.439461100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Version 3: adding SoftMax**\n",
    "\n",
    "Softmax it's also like a normalization operation.\n",
    "So you get the same exact matrix as before, if you apply it on rows that contains -inf like below.\n",
    "This because we will apply the exponential function to each element of the row and we will divide by the sum.\n",
    "So, for example on the first row, the sum that we will get will be:\n",
    "e^0 + e^(-inf) +e^(-inf) + ... = 1 + 0 + 0 + ... = 1\n",
    "While the row will look like:\n",
    "1 0 0 0 0 0 0\n",
    "So if we divide the row by its sum we will just have the same row as before.\n",
    "If we apply it on the 2nd row, we will have 2 as a sum, so we will just have:\n",
    "0.5 0.5 0 0 0 0 0\n",
    "etc...\n",
    "\n",
    "Why is this more interesting than v2?\n",
    "Because we initialize wei as 0. We can see it as the 'interaction strenght', or like a affinity.\n",
    "It's saying how much of each token from the past do we want to aggregate and average up.\n",
    "When we set them to -inf, we are saying that token from the past cannot communicate with token from the future.\n",
    "This is a preview for self-attention, because in the next versions we will take into account the affinity between tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n        [0., 0., 0., 0., 0., 0., -inf, -inf],\n        [0., 0., 0., 0., 0., 0., 0., -inf],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # For all the elements where tril is 0, make them being -inf in the wei matrix\n",
    "wei"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.515258400Z",
     "start_time": "2023-09-04T15:25:42.454421500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = F.softmax(wei, dim=-1) # Take a softmax on every single row.\n",
    "wei"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.516255500Z",
     "start_time": "2023-09-04T15:25:42.470378500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:25:42.516255500Z",
     "start_time": "2023-09-04T15:25:42.498303600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Version 4: self-attention**\n",
    "\n",
    "Now it's time to make each token smart, by using a self-attention matrix. Every token, at each position, will emit two vectors:\n",
    "\n",
    "1. Query vector -> What am I looking for?\n",
    "2. Key vector -> What do I contain?\n",
    "\n",
    "The way we get affinity between these tokens is the dot product between the query vector of my current token and all the other key vectors.\n",
    "That dot product will become `wei`.\n",
    "\n",
    "We'll implement what is called a single head of self-attention.\n",
    "\n",
    "**Notes about self-attention**\n",
    "\n",
    "1. It is like a communication mechanism, where you have a number of nodes in a directed graph, where you have edges pointing between them. Every node has some vector of information, and it gets to aggregate information via a weighted sum from all the nodes that point to it. This is done in a data dependent manner, so depending on the data stored in the nodes.\n",
    "In our case, we will have 8 nodes (because we are using T = 8), and the first node will only point towards itself. The second node is pointed by the first node and by itself, etc...\n",
    "2. There is no notion of space, so attention acts over a set of vectors. So the nodes have no idea where they are positioned in the space. That's why we need to encode them positionally and give them some info that is anchored to a specific position, so that they know where they are. This is different from convolution, because the convolutional filters kind of act like space.\n",
    "3. The elements across the batch dimension, which are independent samples, never talk to each other. So, it's like if we have 4 (because we are using B = 4) different direct graphs that have 8 independent nodes.\n",
    "4. In our direct graph, future tokens will not communicate to the past tokens. This is not necessarily the constraint in the general case. In many cases all the nodes talk to each other, for example with sentiment analysis.\n",
    "5. It's \"self-attention\" because query, key and value are all coming from the same source, that is `x`. In principle, attention is much more general than that, so you can have a case where query comes from x, but keys and values are from a separate source. This is called \"cross-attention\".\n",
    "6. \"Scaled\" attention (from the original paper) additionally divides `wei`by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 8, 32])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "# Forward the linear on top of x\n",
    "# All the tokens in all of the positions in the BxT arrangement, in parallel and independently, produce a key and a query. No communication has happened yet.\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "# Communication will happen here, with the dot product\n",
    "wei = q @ k.transpose(-2 , -1) # Transpose the last two dimension, because we need it for the dot product (and the first dimension is the batch) (B,T,16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # v is the vector that we aggregate, instead of the raw x\n",
    "out = wei @ v\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:49:32.046634800Z",
     "start_time": "2023-09-04T15:49:32.004748500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T15:50:05.413817600Z",
     "start_time": "2023-09-04T15:50:05.394845300Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
